{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Manual fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do manual curve fits to the submodel using steady-state and activation curves for pow1 models. The curves were extracted by Chaitanya (using pyNeuroML) and are in a pickle in `support_files/manualfit_curves_complete_df.pkl`. \n",
    "\n",
    "This notebook does manual parameter fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x,a,b):\n",
    "    return 1/(1+np.exp(-a*x+b))\n",
    "\n",
    "def tau_fun(x,a,b,c,d,e,f):\n",
    "    y = (x - a)\n",
    "    return np.exp(b)/(np.exp(-(np.log(c)*y+np.log(d)*y**2)) + np.exp(np.log(e)*y+np.log(f)*y**2))\n",
    "\n",
    "tau_fit_fcn = tau_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most models have 61 data points\n",
    "df = pickle.load(open('./support_files/manualfit_curves_complete_df.pkl','rb'))\n",
    "\n",
    "kvactonly = pickle.load(open('./support_files/kvact_only.pkl', 'rb'))\n",
    "pow1 = kvactonly['pow1']\n",
    "pow4 = kvactonly['pow4']\n",
    "\n",
    "dats = {}\n",
    "\n",
    "infs = []\n",
    "taus = []\n",
    "names = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "    if row['Name'].split('/')[2] == 'K' and row['Name'].split('/')[3] in pow1:       \n",
    "        name = row['Name'].split('/')[3]\n",
    "        if name not in dats:\n",
    "            dats[name] = {}\n",
    "        \n",
    "        trace = row['Trace'][0].reshape(-1)\n",
    "        curve_type = row['Name'][-7:-4]\n",
    "        V = np.linspace(-150,150,61)\n",
    "        \n",
    "        try:\n",
    "            if curve_type == 'inf':\n",
    "                popt_inf_ls, _ = curve_fit(sigmoid, V, trace)\n",
    "                dats[name]['inf'] = [popt_inf_ls]\n",
    "                \n",
    "            else:\n",
    "                p0 = [V[np.argmax(trace)], np.log(np.max(trace)), np.exp(0.5), np.exp(0), np.exp(0.5), np.exp(0)]\n",
    "                \n",
    "                popt_tau_ls, _ = curve_fit(tau_fun, V, trace, p0=p0)        \n",
    "                dats[name]['tau'] = [popt_tau_ls]\n",
    "\n",
    "        except:\n",
    "            print('err processing')\n",
    "            print(row)\n",
    "            taus += [popt_tau_ls]\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the results to a pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dats is a dict (keys are channel names) of dicts (keys are inf/tau)\n",
    "pickle.dump(dats, open('./support_files/manualfit_params.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual fits successful for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dats.keys()) - np.sum([len(dats[k].keys()) != 2 for k in dats.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "channel models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating responses using manually fitted parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simulate manually fitted parameters using the OmniModel and compute correlation coefficients between simulated and observed traces. Plots are generated for each of those simulations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from box import Box\n",
    "from delfi.distribution import TransformedNormal\n",
    "from delfi.utils.viz import plot_pdf\n",
    "from model.ChannelOmni import ChannelOmni\n",
    "from model.ChannelOmniStats import ChannelOmniStats as ChannelStats\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from support_files.pickle_macos import pickle_load\n",
    "\n",
    "import sys; sys.path.append('../')\n",
    "from common import col, svg, samples_nd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "m = ChannelOmni()\n",
    "s = ChannelStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mats = Box(pickle_load('./support_files/pow1_mats_comp_lfs.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_chans = mats['ap']['data'].shape[0]  # 372"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_chans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "for idx_chan in tqdm(range(N_chans)):\n",
    "    datum = {}\n",
    "    datum['idx_mats'] = idx_chan\n",
    "    \n",
    "    name_gt = mats['ap']['names'][idx_chan]   \n",
    "    datum['name'] = name_gt\n",
    "\n",
    "    try:\n",
    "        params_manual_inf = dats[name_gt + '.mod']['inf'][0]\n",
    "        params_manual_tau = dats[name_gt + '.mod']['tau'][0]\n",
    "        params_manual = np.hstack((params_manual_inf, params_manual_tau)).reshape(-1)\n",
    "        datum['params_manual'] = params_manual\n",
    "    except:\n",
    "        print('no params found for {}'.format(name_gt))\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        trace_manual = m.gen_single(params_manual)\n",
    "        #datum['traces_manual'] = trace_manual\n",
    "\n",
    "        # gt trace simulated with neuron\n",
    "        # note that we are introducing an offset to make length of data match\n",
    "        # using 6: for now, alternatively could do :-6\n",
    "        trace_gt = {\n",
    "            'v_act':   {'data' : mats['act']['data'][idx_chan,   6:, 1:].T},\n",
    "            'v_inact': {'data' : mats['inact']['data'][idx_chan, 6:, 1:].T},\n",
    "            'v_deact': {'data' : mats['deact']['data'][idx_chan, 6:, 1:].T},\n",
    "            'v_ap':    {'data' : mats['ap']['data'][idx_chan,    6:, 1:].T},\n",
    "            'v_ramp':  {'data' : mats['ramp']['data'][idx_chan,  6:, 1:].T},\n",
    "        }\n",
    "        #datum['traces_gt'] = trace_gt\n",
    "\n",
    "        # concat'ed timeseries\n",
    "        trace_manual_concat = np.concatenate((\n",
    "            trace_manual['v_act']['data'].reshape(-1),\n",
    "            trace_manual['v_inact']['data'].reshape(-1),\n",
    "            trace_manual['v_deact']['data'].reshape(-1),\n",
    "            trace_manual['v_ap']['data'].reshape(-1),\n",
    "            trace_manual['v_ramp']['data'].reshape(-1)\n",
    "        ))\n",
    "        trace_gt_concat = np.concatenate((\n",
    "            trace_gt['v_act']['data'].reshape(-1),\n",
    "            trace_gt['v_inact']['data'].reshape(-1),\n",
    "            trace_gt['v_deact']['data'].reshape(-1),\n",
    "            trace_gt['v_ap']['data'].reshape(-1),\n",
    "            trace_gt['v_ramp']['data'].reshape(-1)\n",
    "        ))\n",
    "\n",
    "        # calculate corrcoef\n",
    "        corrcoef = np.corrcoef(trace_manual_concat, trace_gt_concat)[0,1]\n",
    "        datum['cc_manual'] = corrcoef\n",
    "\n",
    "        # calculate L2\n",
    "        stats_gt = s.calc([trace_gt])\n",
    "        stats_manual = s.calc([trace_manual])\n",
    "        l2 = np.linalg.norm(stats_gt-stats_manual)\n",
    "        datum['l2_manual'] = l2\n",
    "\n",
    "        dataset.append(datum)\n",
    "    \n",
    "    except:\n",
    "        print('error with : {}'.format(idx_chan))\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset\n",
    "pickle.dump(dataset, open('./results/manual_fits_lfs.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# range of manual params\n",
    "params_manual = []\n",
    "ccs = []\n",
    "for i in range(len(dataset)):\n",
    "    if dataset[i]['cc_manual'] > 0.9:\n",
    "        params_manual.append(dataset[i]['params_manual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(params_manual).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    if dataset[i]['cc_manual'] > 0.9:\n",
    "\n",
    "        # plot \n",
    "        protocols = ['ap', 'act', 'inact', 'deact', 'ramp']\n",
    "\n",
    "        with mpl.rc_context(fname='../.matplotlibrc'):\n",
    "            plt.figure(figsize=(20/2.54, 5/2.54))\n",
    "\n",
    "            for p, protocol in enumerate(protocols):    \n",
    "                if protocol == 'ap':\n",
    "                    ds = 10\n",
    "                else:\n",
    "                    ds = 100\n",
    "\n",
    "                plt.subplot(2, 5, p+1)\n",
    "\n",
    "                name_gt = dataset[i]['name']\n",
    "                corrcoef = dataset[i]['cc_manual']\n",
    "                \n",
    "                if p == 0:\n",
    "                    plt.gca().set_title('Channel {} · cc={:.5f}'.format(name_gt, corrcoef), \n",
    "                                        loc='left', pad=15,\n",
    "                                        fontdict={'fontsize': 10})  # · $L_2$-error {:.2f}\n",
    "\n",
    "                trace_gt = dataset[i]['traces_gt']\n",
    "                trace_manual = dataset[i]['traces_manual']\n",
    "                \n",
    "                N = trace_gt['v_' + protocol]['data'].shape[0]\n",
    "                if N == 1:\n",
    "                    plt.gca().set_prop_cycle('color',[plt.cm.Blues_r(i) for i in np.linspace(0., 1, N)])\n",
    "                else:\n",
    "                    plt.gca().set_prop_cycle('color',[plt.cm.Blues(i) for i in np.linspace(0.3, 1, N)])\n",
    "\n",
    "                plt.plot(trace_manual['v_' + protocol]['time'][::ds], \n",
    "                         trace_gt['v_' + protocol]['data'].T[::ds], \n",
    "                         linewidth=1.);  # usually 1.5\n",
    "\n",
    "                #plt.xlim([0, mat[-1,0]])\n",
    "                plt.xticks([])\n",
    "\n",
    "                plt.yticks([])\n",
    "\n",
    "                sns.despine(left=True, bottom=True, offset=5)\n",
    "\n",
    "            for p, protocol in enumerate(protocols):    \n",
    "                if protocol == 'ap':\n",
    "                    ds = 10\n",
    "                else:\n",
    "                    ds = 100\n",
    "\n",
    "                plt.subplot(2, 5, p+6)\n",
    "\n",
    "                N = trace_manual['v_' + protocol]['data'].shape[0]\n",
    "                if N == 1:\n",
    "                    plt.gca().set_prop_cycle('color',[plt.cm.Greys_r(i) for i in np.linspace(0., 1, N)])\n",
    "                else:\n",
    "                    plt.gca().set_prop_cycle('color',[plt.cm.Greys(i) for i in np.linspace(0.3, 1, N)])\n",
    "\n",
    "                plt.plot(trace_manual['v_' + protocol]['time'][::ds], \n",
    "                         trace_manual['v_' + protocol]['data'].T[::ds], \n",
    "                         linewidth=1., alpha=1.0);  # usually 1.5\n",
    "\n",
    "                #plt.xlim([0, mat[-1,0]])\n",
    "                plt.xticks([])\n",
    "\n",
    "                plt.yticks([])\n",
    "\n",
    "                sns.despine(left=True, bottom=True, offset=5)\n",
    "                #plt.axis('off')\n",
    "\n",
    "                plt.plot([0., 100.],[-0.1, -0.1], color='k', linewidth=2)\n",
    "                plt.text(0.0, -0.4, '100ms', fontsize=8)\n",
    "\n",
    "            !mkdir -p results\n",
    "            !mkdir -p results/manual_fit/\n",
    "            !mkdir -p results/manual_fit/svg\n",
    "            \n",
    "            plt.savefig('./results/manual_fit/svg/{}.svg'.format(i))\n",
    "            plt.close()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
