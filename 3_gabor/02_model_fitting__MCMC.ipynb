{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov-Chain Monte Carlo sampler\n",
    "- the Gabor-GLM has a tractable likelihood\n",
    "- the non-linear parametrization of the Gabor RF prohibits standard GLM methods for obtaining parameter estimates and posteriors\n",
    "- the likelihood can still be used for MCMC sampling to obtain a (asysmptotically correct) posterior estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import delfi.distribution as dd\n",
    "import delfi.generator as dg\n",
    "import delfi.inference as infer\n",
    "import delfi.utils.io as io\n",
    "from delfi.utils.viz import plot_pdf\n",
    "\n",
    "from model.gabor_rf import maprf as model\n",
    "from model.gabor_stats import maprfStats\n",
    "from utils import setup_sim, setup_sampler, quick_plot, contour_draws, get_data_o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameters for this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "seed = 42    # seed for generation of xo for selected cell. MCMC currently not seeded ! \n",
    "\n",
    "idx_cell = 6 # load toy cell number 6 (cosine-shaped RF with 1Hz firing rate)\n",
    "\n",
    "fix_position=True         # fixues RF position during sampling to (0,0)\n",
    "parametrization='logit_φ' # chosen parameterization of Gabor (affects priors !) \n",
    "\n",
    "n_samples = 1000000  # number of MCMC samples\n",
    "\n",
    "savefile = 'results/MCMC/toycell_' + str(idx_cell) + '/maprf_MCMC_prior01_run_1_'+ str(n_samples)+'samples_param9_5min'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load cell, generate xo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observation, models\n",
    "reload_obs_stats = False\n",
    "\n",
    "if reload_obs_stats:\n",
    "    gtd = np.load('results/SNPE/toycell_6/ground_truth_data.npy', allow_pickle=True)[()]\n",
    "    obs_stats = gtd['obs_stats']\n",
    "    sim_info = np.load('results/sim_info.npy', allow_pickle=True)[()]\n",
    "    d, params_ls = sim_info['d'], sim_info['params_ls']\n",
    "    p = get_maprf_prior_01(params_ls)\n",
    "    import delfi.generator as dg\n",
    "    g = dg.Default(model=None, prior=p[0], summary=None)\n",
    "else:\n",
    "    import theano\n",
    "\n",
    "    # result dirs\n",
    "    !mkdir -p results/\n",
    "    !mkdir -p results/SNPE/\n",
    "    !mkdir -p results/SNPE/toycell_6/\n",
    "\n",
    "    # training data and true parameters, data, statistics\n",
    "    idx_cell = 6 # load toy cell number 6 (cosine-shaped RF with 1Hz firing rate)\n",
    "    filename = 'results/toy_cells/toy_cell_' + str(idx_cell) + '.npy'\n",
    "\n",
    "    g, prior, d = setup_sim(seed, path='')    \n",
    "    g.model.rng = np.random.RandomState(seed=seed)\n",
    "    params_dict_true = np.load(filename, allow_pickle=True)[()]\n",
    "    params_dict_true['kernel']['t']['value'] = np.cast[theano.config.floatX](1.)\n",
    "    g.model.params_dict = params_dict_true.copy()\n",
    "    obs = g.model.gen_single()\n",
    "    \n",
    "    g, prior, d = setup_sim(seed, path='')    \n",
    "    obs_stats, pars_true = get_data_o(filename, g, seed)\n",
    "    \n",
    "    assert np.all(g.summary.calc([obs]) == obs_stats)\n",
    "    \n",
    "    rf = g.model.params_to_rf(pars_true)[0]    \n",
    "    \n",
    "    # plot ground-truth receptive field\n",
    "    plt.imshow(rf, interpolation='None')\n",
    "    plt.show()\n",
    "    obs_stats, obs_stats[0,-1] # summary statistics: (STA , spike count (over 5 minutes simulation) )\n",
    "\n",
    "    np.save('results/SNPE/toycell_6/ground_truth_data',\n",
    "            {'obs_stats' : obs_stats, 'pars_true' : pars_true, 'rf' : rf})\n",
    "\n",
    "    # visualize RFs defined by prior-drawn parameters theta\n",
    "    contour_draws(g.prior, g, obs_stats, d=d)\n",
    "    print(obs_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up MCMC sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference, data = setup_sampler(prior, obs, d, g, params_dict=params_dict_true, \n",
    "                          fix_position=False, parametrization='logit_φ')\n",
    "\n",
    "inference.samplers[0].mu['logit_xo'] = prior['logit_xo']['mu'][0]\n",
    "inference.samplers[0].mu['logit_yo'] = prior['logit_yo']['mu'][0]\n",
    "\n",
    "inference.samplers[0].sd['logit_xo'] = prior['logit_xo']['sigma'][0]\n",
    "inference.samplers[0].sd['logit_yo'] = prior['logit_yo']['sigma'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample RF parameters (with Poisson bias marginalized out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T, L = inference.sample(n_samples)\n",
    "T = {k.name: t for k, t in T.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually back-transform remaining parameters (location)\n",
    "def scaled_expit_i(v):\n",
    "    return 2. / (1. + np.exp(-v)) - 1\n",
    "\n",
    "T['xo'], T['yo'] = scaled_expit_i(T['logit_xo']), scaled_expit_i(T['logit_yo'])\n",
    "\n",
    "x,y = T['xo'], T['yo']\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.subplot(221)\n",
    "plt.plot(x[0:])\n",
    "plt.plot(y[0:])\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.hist(x[0:], alpha=0.5, normed=True)\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.hist(y[0:], alpha=0.5, normed=True)\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(x[0:], y[0:], '.k', alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample Poisson bias (conditioned on the others)\n",
    "- above sampler did not sample biases, but instead marginalized them out during sampling (much faster mixing)\n",
    "- conditionally on all other parameters, we can still sample the biases\n",
    "- note this only works for an exponential distribution as prior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference.sample_biases(data, T, g.model.dt)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(T['bias'])\n",
    "print('mean: ' + str(T['bias'].mean()) + ', var: ' + str(T['bias'].var()))\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(T['λo'])\n",
    "print('mean: ' + str(T['λo'].mean()) + ', var: ' + str(T['λo'].var()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example posterior draws (in direct comparison with xo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(16,12))\n",
    "i = 1\n",
    "for t in np.sort(np.random.choice(T['gain'].shape[0], 12, replace=False)):\n",
    "    params_dict = {'kernel' : {'s' : {}, 'l' : {}}, 'glm': {}}\n",
    "    params_dict['glm']['bias'] = T['bias'][t]\n",
    "    params_dict['kernel']['s']['phase'] = T['phase'][t]\n",
    "    params_dict['kernel']['s']['angle'] = T['angle'][t] \n",
    "    params_dict['kernel']['s']['freq']  = T['freq'][t]\n",
    "    params_dict['kernel']['s']['ratio'] = T['ratio'][t]\n",
    "    params_dict['kernel']['s']['width'] = T['width'][t]\n",
    "    params_dict['kernel']['s']['gain'] = T['gain'][t]\n",
    "    params_dict['kernel']['l']['xo'] = T['xo'][t]\n",
    "    params_dict['kernel']['l']['yo'] = T['yo'][t]\n",
    "\n",
    "    axis_x = g.model.axis_x - params_dict['kernel']['l']['xo']\n",
    "    axis_y = g.model.axis_y - params_dict['kernel']['l']['yo']    \n",
    "    g.model._gen.grid_x, g.model._gen.grid_y = np.meshgrid(axis_x, axis_y)    \n",
    "    \n",
    "    ks = g.model._eval_ks(bias=params_dict['glm']['bias'], \n",
    "                    angle=params_dict['kernel']['s']['angle'],\n",
    "                    freq=params_dict['kernel']['s']['freq'],\n",
    "                    gain=params_dict['kernel']['s']['gain'],\n",
    "                    phase=params_dict['kernel']['s']['phase'],\n",
    "                    ratio=params_dict['kernel']['s']['ratio'],\n",
    "                    width=params_dict['kernel']['s']['width'])\n",
    "    \n",
    "    plt.subplot(3,4,i)\n",
    "    plt.imshow(np.hstack((ks.reshape(d,d), g.model.params_to_rf(pars_true)[0])), interpolation='None')\n",
    "    plt.title('t =' + str(t))\n",
    "    \n",
    "    print('loc:' , [T['xo'][t], T['yo'][t]])    \n",
    "    i += 1\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# marginal histograms for each (transformed) parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "burnin = 50\n",
    "\n",
    "for key in ['bias', 'λo', \n",
    "            'gain', 'log_A', 'phase', 'logit_φ',\n",
    "            'angle', 'logit_θ', 'freq', 'log_f',\n",
    "            'ratio', 'width', 'log_γ', 'log_b', \n",
    "            'xo', 'yo', 'logit_xo', 'logit_yo'\n",
    "            ]:\n",
    "    \n",
    "    if key in T.keys():\n",
    "        x = T[key][burnin:]\n",
    "        plt.hist(x, bins=np.linspace(x.min(), x.max(), 20), alpha=0.5, normed=True)\n",
    "        plt.title(key)\n",
    "        plt.show()\n",
    "        print('mean:', x.mean())\n",
    "        print('var:', x.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# posterior samples versus prior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## actual parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "samples = np.hstack([np.atleast_2d(T[key].T).T for key in ['bias', 'gain', 'phase', 'freq','angle','ratio','width', 'xo', 'yo']])\n",
    "\n",
    "pars_raw = np.array([ params_dict_true['glm']['bias'],\n",
    "                      params_dict_true['kernel']['s']['gain'],\n",
    "                      params_dict_true['kernel']['s']['phase'],\n",
    "                      params_dict_true['kernel']['s']['freq'],\n",
    "                      params_dict_true['kernel']['s']['angle'],\n",
    "                      params_dict_true['kernel']['s']['ratio'],\n",
    "                      params_dict_true['kernel']['s']['width'],\n",
    "                      params_dict_true['kernel']['l']['xo'],\n",
    "                      params_dict_true['kernel']['l']['yo'] ])\n",
    "\n",
    "plot_pdf(g.prior, lims=[-3,3], gt=pars_raw.reshape(-1), figsize=(16,16), resolution=100, samples=samples.T,\n",
    "         ticks=True, labels_params=['bias', 'gain', 'phase', 'freq', 'angle', 'ratio', 'width', 'xo', 'yo']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameters in log/logit space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "samples = np.hstack([np.atleast_2d(T[key].T).T for key in ['bias', 'log_A', 'logit_φ', 'log_f','logit_θ','log_γ','log_b', 'logit_xo', 'logit_yo']])\n",
    "\n",
    "plot_pdf(g.prior, lims=[-3,3], gt=pars_true.reshape(-1), figsize=(16,16), resolution=100, samples=samples.T,\n",
    "         ticks=True, labels_params=['bias', 'gain', 'logit phase', 'log freq', 'logit angle', 'log ratio', 'log width', 'logit xo', 'logit yo']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (roughly) check for mixing of the chain\n",
    "- the MCMC sampler will have trouble to mix between the different distinct posterior modes\n",
    "- since we know the symmetries of the Gabor parametrization that cause the multimodality, we can later just 'mirror' the samples to obtain all posterior modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,5) )\n",
    "plt.plot(samples)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "np.save(savefile, {'T' : T, 'params_dict_true' : params_dict_true})"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
